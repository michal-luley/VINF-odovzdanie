{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595ac8de",
   "metadata": {},
   "source": [
    "# Vyhľadávanie informácii - projekt - Michal Lüley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7446aa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# File system management\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark import jars\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b97340",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FILE_PATH = \"articles1.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153376a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"VINF_disease_searching_luley_michal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02709a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up home directory\n",
    "D_RAW_DIR = os.path.realpath(os.path.join(os.path.dirname(\"VINF\"), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d1b43",
   "metadata": {},
   "source": [
    "# Loading essential files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83beee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to files \n",
    "main_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'source_files', 'main_document.csv'))\n",
    "countries_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'countries.csv'))\n",
    "symptoms_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'symptoms.csv'))\n",
    "transmissions_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'transmision.csv'))\n",
    "\n",
    "# Loading csv files for maintenance\n",
    "main_df = pd.read_csv(main_document_path, sep='\\t')\n",
    "countries_df = pd.read_csv(countries_document_path)\n",
    "symptoms_df = pd.read_csv(symptoms_document_path)\n",
    "transmissions_df = pd.read_csv(transmissions_document_path)\n",
    "\n",
    "# Filling null values with empty string to able to append items\n",
    "main_df = main_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e777a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining regex patterns to be used for regex searching\n",
    "diseases_pattern = re.compile(('|'.join(main_df['Disease'])).replace(\"(\", \"\\(\").replace(\")\", \"\\)\"), re.IGNORECASE)\n",
    "diseases_type_pattern = re.compile('bacteria|virus|viral', re.IGNORECASE)\n",
    "symptoms_pattern = re.compile('|'.join(symptoms_df['Symptom']), re.IGNORECASE)\n",
    "countries_pattern = re.compile('|'.join(countries_df['Country']), re.IGNORECASE)\n",
    "transmission_pattern = re.compile('|'.join(transmissions_df['Transmission']), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a15be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading source unzipped xml file\n",
    "dfs = spark.read \\\n",
    "    .format('com.databricks.spark.xml') \\\n",
    "    .option(\"rowTag\", \"page\") \\\n",
    "    .load(os.path.join(D_RAW_DIR, 'input_file', SOURCE_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049ba094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting necessary columns from page, so title and text and filtering importatn pages\n",
    "from pyspark.sql.functions import col\n",
    "dfs_valuable = dfs.select([\"title\", \"revision.text._VALUE\"]) \\\n",
    "    .filter(~col(\"revision.text._VALUE\").like('#REDIRECT%')) \\\n",
    "    .filter(col(\"revision.text._VALUE\").rlike('(?i)bacteria|(?i)virus|(?i)viral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa62ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Defining udf for finding matches between page and item(disease, symptom, transmission, country)\n",
    "def get_matching_string(line, regex):\n",
    "    matches = list(set([str(x).lower() for x in re.findall(regex, line)]))\n",
    "    return matches if matches else None\n",
    "\n",
    "# Defining udf for finding matches between page and disease type\n",
    "def get_matching_string_type(line, regex):\n",
    "    disease_types = re.findall(regex, line)    \n",
    "    match = max([x.lower().replace(\"viral\", \"virus\") for x in disease_types], key=disease_types.count) \n",
    "    return match if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbccdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring UDF functions\n",
    "from pyspark.sql.functions import udf\n",
    "udf_func_type = udf(lambda line :get_matching_string_type(line, diseases_type_pattern), StringType())\n",
    "udf_func_diseases = udf(lambda line :get_matching_string(line, diseases_pattern), ArrayType(StringType()))\n",
    "udf_func_symptoms = udf(lambda line :get_matching_string(line, symptoms_pattern), ArrayType(StringType()))\n",
    "udf_func_countries = udf(lambda line :get_matching_string(line, countries_pattern), ArrayType(StringType()))\n",
    "udf_func_transmissions = udf(lambda line :get_matching_string(line, transmission_pattern), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f6fcd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating collumns disease, type, symptoms, countries, transmissions and dropping yet \n",
    "# unnecessary columns _VALUE and title\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "dfs_final = dfs_valuable \\\n",
    "                .withColumn(\"disease\", udf_func_diseases(col('title'))[0]) \\\n",
    "                .na.drop(subset=[\"disease\"]) \\\n",
    "                .withColumn(\"type\", udf_func_type('_VALUE')) \\\n",
    "                .withColumn(\"symptoms\", udf_func_symptoms(col('_VALUE'))) \\\n",
    "                .withColumn(\"countries\", udf_func_countries(col('_VALUE'))) \\\n",
    "                .withColumn(\"transmissions\", udf_func_transmissions(col('_VALUE'))) \\\n",
    "                .drop(col(\"_VALUE\")) \\\n",
    "                .drop(col(\"title\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe12c158",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for row in dfs_final.rdd.collect():\n",
    "    # Getting id of row from main document to know, which line(disease) will be processed\n",
    "    id = main_df[main_df['Disease'].str.contains(row.disease.replace(\"(\", \"\\(\").replace(\")\", \"\\)\"), flags=re.IGNORECASE)].index[0]\n",
    "\n",
    "    # Writing type of disease to main document\n",
    "    main_df.at[id, 'Type'] = f\"{row.type}\"\n",
    "\n",
    "    # Writing symptoms of disease to main document\n",
    "    if row.symptoms != None:\n",
    "        for symptom in row.symptoms:\n",
    "            if main_df.at[id, 'Symptom'] == \"\":\n",
    "                main_df.at[id, 'Symptom'] = f\"{str(symptom)}\"\n",
    "            else:\n",
    "                if symptom not in main_df['Symptom'][id]:\n",
    "                    main_df.at[id, 'Symptom'] = f\"{main_df['Symptom'][id]}; {str(symptom)}\"\n",
    "\n",
    "    # Writing countries of disease spread to main document \n",
    "    if row.countries != None:\n",
    "        for country in row.countries:\n",
    "            if main_df.at[id, 'Country'] == \"\":\n",
    "                main_df.at[id, 'Country'] = f\"{str(country)}\"\n",
    "            else:\n",
    "                #print(type(main_df['Country'][id]))\n",
    "                if country not in main_df['Country'][id]:\n",
    "                    main_df.at[id, 'Country'] = f\"{main_df['Country'][id]}; {str(country)}\"\n",
    "\n",
    "    # Writing transmissions of disease to main document    \n",
    "    if row.transmissions != None:\n",
    "        for transmission in row.transmissions:\n",
    "            if main_df.at[id, 'Transmission'] == \"\":\n",
    "                main_df.at[id, 'Transmission'] = f\"{str(transmission)}\"\n",
    "            else:\n",
    "                if transmission not in main_df['Transmission'][id]:\n",
    "                    main_df.at[id, 'Transmission'] = f\"{main_df['Transmission'][id]}; {str(transmission)}\"         \n",
    "\n",
    "\n",
    "\n",
    "    main_df.to_csv(main_document_path, sep='\\t', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinfenv_kernel",
   "language": "python",
   "name": "vinfenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
