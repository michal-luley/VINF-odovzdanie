{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595ac8de",
   "metadata": {},
   "source": [
    "# Vyhľadávanie informácii - projekt - Michal Lüley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7446aa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# File system management\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark import jars\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b97340",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FILE_PATH = \"articles1.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153376a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"D:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Program Files\\Python38\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYSPARK_PYTHON\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYSPARK_DRIVER_PYTHON\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[1;32m----> 7\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVINF_disease_searching_luley_michal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pyspark\\context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pyspark\\context.py:284\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconf())\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Create a single Accumulator in Java that we'll send all our updates through;\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# they will be passed back to us through a TCP server\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\py4j\\java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setting up spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"VINF_disease_searching_luley_michal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02709a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up home directory\n",
    "D_RAW_DIR = os.path.realpath(os.path.join(os.path.dirname(\"VINF\"), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d1b43",
   "metadata": {},
   "source": [
    "# Loading essential files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83beee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to files \n",
    "main_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'source_files', 'main_document.csv'))\n",
    "countries_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'countries.csv'))\n",
    "symptoms_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'symptoms.csv'))\n",
    "transmissions_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'transmision.csv'))\n",
    "\n",
    "# Loading csv files for maintenance\n",
    "main_df = pd.read_csv(main_document_path, sep='\\t')\n",
    "countries_df = pd.read_csv(countries_document_path)\n",
    "symptoms_df = pd.read_csv(symptoms_document_path)\n",
    "transmissions_df = pd.read_csv(transmissions_document_path)\n",
    "\n",
    "# Filling null values with empty string to able to append items\n",
    "main_df = main_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e777a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining regex patterns to be used for regex searching\n",
    "diseases_pattern = re.compile(('|'.join(main_df['Disease'])).replace(\"(\", \"\\(\").replace(\")\", \"\\)\"), re.IGNORECASE)\n",
    "diseases_type_pattern = re.compile('bacteria|virus|viral', re.IGNORECASE)\n",
    "symptoms_pattern = re.compile('|'.join(symptoms_df['Symptom']), re.IGNORECASE)\n",
    "countries_pattern = re.compile('|'.join(countries_df['Country']), re.IGNORECASE)\n",
    "transmission_pattern = re.compile('|'.join(transmissions_df['Transmission']), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading source unzipped xml file\n",
    "dfs = spark.read \\\n",
    "    .format('com.databricks.spark.xml') \\\n",
    "    .option(\"rowTag\", \"page\") \\\n",
    "    .load(os.path.join(D_RAW_DIR, 'input_file', SOURCE_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ba094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting necessary columns from page, so title and text and filtering importatn pages\n",
    "from pyspark.sql.functions import col\n",
    "dfs_valuable = dfs.select([\"title\", \"revision.text._VALUE\"]) \\\n",
    "    .filter(~col(\"revision.text._VALUE\").like('#REDIRECT%')) \\\n",
    "    .filter(col(\"revision.text._VALUE\").rlike('(?i)bacteria|(?i)virus|(?i)viral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Defining udf for finding matches between page and item(disease, symptom, transmission, country)\n",
    "def get_matching_string(line, regex):\n",
    "    matches = list(set([str(x).lower() for x in re.findall(regex, line)]))\n",
    "    return matches if matches else None\n",
    "\n",
    "# Defining udf for finding matches between page and disease type\n",
    "def get_matching_string_type(line, regex):\n",
    "    disease_types = re.findall(regex, line)    \n",
    "    match = max([x.lower().replace(\"viral\", \"virus\") for x in disease_types], key=disease_types.count) \n",
    "    return match if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbccdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring UDF functions\n",
    "from pyspark.sql.functions import udf\n",
    "udf_func_type = udf(lambda line :get_matching_string_type(line, diseases_type_pattern), StringType())\n",
    "udf_func_diseases = udf(lambda line :get_matching_string(line, diseases_pattern), ArrayType(StringType()))\n",
    "udf_func_symptoms = udf(lambda line :get_matching_string(line, symptoms_pattern), ArrayType(StringType()))\n",
    "udf_func_countries = udf(lambda line :get_matching_string(line, countries_pattern), ArrayType(StringType()))\n",
    "udf_func_transmissions = udf(lambda line :get_matching_string(line, transmission_pattern), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6fcd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating collumns disease, type, symptoms, countries, transmissions and dropping yet \n",
    "# unnecessary columns _VALUE and title\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "dfs_final = dfs_valuable \\\n",
    "                .withColumn(\"disease\", udf_func_diseases(col('title'))[0]) \\\n",
    "                .na.drop(subset=[\"disease\"]) \\\n",
    "                .withColumn(\"type\", udf_func_type('_VALUE')) \\\n",
    "                .withColumn(\"symptoms\", udf_func_symptoms(col('_VALUE'))) \\\n",
    "                .withColumn(\"countries\", udf_func_countries(col('_VALUE'))) \\\n",
    "                .withColumn(\"transmissions\", udf_func_transmissions(col('_VALUE'))) \\\n",
    "                .drop(col(\"_VALUE\")) \\\n",
    "                .drop(col(\"title\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12c158",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for row in dfs_final.rdd.collect():\n",
    "    # Getting id of row from main document to know, which line(disease) will be processed\n",
    "    id = main_df[main_df['Disease'].str.contains(row.disease.replace(\"(\", \"\\(\").replace(\")\", \"\\)\"), flags=re.IGNORECASE)].index[0]\n",
    "\n",
    "    # Writing type of disease to main document\n",
    "    main_df.at[id, 'Type'] = f\"{row.type}\"\n",
    "\n",
    "    # Writing symptoms of disease to main document\n",
    "    if row.symptoms != None:\n",
    "        for symptom in row.symptoms:\n",
    "            if main_df.at[id, 'Symptom'] == \"\":\n",
    "                main_df.at[id, 'Symptom'] = f\"{str(symptom)}\"\n",
    "            else:\n",
    "                if symptom not in main_df['Symptom'][id]:\n",
    "                    main_df.at[id, 'Symptom'] = f\"{main_df['Symptom'][id]}; {str(symptom)}\"\n",
    "\n",
    "    # Writing countries of disease spread to main document \n",
    "    if row.countries != None:\n",
    "        for country in row.countries:\n",
    "            if main_df.at[id, 'Country'] == \"\":\n",
    "                main_df.at[id, 'Country'] = f\"{str(country)}\"\n",
    "            else:\n",
    "                #print(type(main_df['Country'][id]))\n",
    "                if country not in main_df['Country'][id]:\n",
    "                    main_df.at[id, 'Country'] = f\"{main_df['Country'][id]}; {str(country)}\"\n",
    "\n",
    "    # Writing transmissions of disease to main document    \n",
    "    if row.transmissions != None:\n",
    "        for transmission in row.transmissions:\n",
    "            if main_df.at[id, 'Transmission'] == \"\":\n",
    "                main_df.at[id, 'Transmission'] = f\"{str(transmission)}\"\n",
    "            else:\n",
    "                if transmission not in main_df['Transmission'][id]:\n",
    "                    main_df.at[id, 'Transmission'] = f\"{main_df['Transmission'][id]}; {str(transmission)}\"         \n",
    "\n",
    "\n",
    "\n",
    "    main_df.to_csv(main_document_path, sep='\\t', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinfenv_kernel",
   "language": "python",
   "name": "vinfenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
