{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595ac8de",
   "metadata": {},
   "source": [
    "# Vyhľadávanie informácii - projekt - Michal Lüley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7446aa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# File system management\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark import jars\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b97340",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FILE_PATH = \"articles1.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153376a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"VINF_disease_searching_luley_michal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02709a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up home directory\n",
    "D_RAW_DIR = os.path.realpath(os.path.join(os.path.dirname(\"VINF\"), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d1b43",
   "metadata": {},
   "source": [
    "# Loading essential files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83beee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to files \n",
    "main_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'source_files', 'main_document.csv'))\n",
    "countries_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'countries.csv'))\n",
    "symptoms_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'symptoms.csv'))\n",
    "transmissions_document_path = os.path.realpath(os.path.join(D_RAW_DIR, 'search_keys', 'transmision.csv'))\n",
    "\n",
    "# Loading csv files for maintenance\n",
    "main_df = pd.read_csv(main_document_path, sep='\\t')\n",
    "countries_df = pd.read_csv(countries_document_path)\n",
    "symptoms_df = pd.read_csv(symptoms_document_path)\n",
    "transmissions_df = pd.read_csv(transmissions_document_path)\n",
    "\n",
    "# Filling null values with empty string to able to append items\n",
    "main_df = main_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e777a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Disease'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Disease'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Defining regex patterns to be used for regex searching\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m diseases_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mmain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDisease\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m), re\u001b[38;5;241m.\u001b[39mIGNORECASE)\n\u001b[0;32m      3\u001b[0m diseases_type_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbacteria|virus|viral\u001b[39m\u001b[38;5;124m'\u001b[39m, re\u001b[38;5;241m.\u001b[39mIGNORECASE)\n\u001b[0;32m      4\u001b[0m symptoms_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(symptoms_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptom\u001b[39m\u001b[38;5;124m'\u001b[39m]), re\u001b[38;5;241m.\u001b[39mIGNORECASE)\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pandas\\core\\frame.py:3455\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3455\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3457\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mD:\\FIIT\\FIIT_skola\\4_grade\\VINF\\source_code\\vinfenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Disease'"
     ]
    }
   ],
   "source": [
    "# Defining regex patterns to be used for regex searching\n",
    "diseases_pattern = re.compile(('|'.join(main_df['Disease'])).replace(\"(\", \"\\(\").replace(\")\", \"\\)\"), re.IGNORECASE)\n",
    "diseases_type_pattern = re.compile('bacteria|virus|viral', re.IGNORECASE)\n",
    "symptoms_pattern = re.compile('|'.join(symptoms_df['Symptom']), re.IGNORECASE)\n",
    "countries_pattern = re.compile('|'.join(countries_df['Country']), re.IGNORECASE)\n",
    "transmission_pattern = re.compile('|'.join(transmissions_df['Transmission']), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading source unzipped xml file\n",
    "dfs = spark.read \\\n",
    "    .format('com.databricks.spark.xml') \\\n",
    "    .option(\"rowTag\", \"page\") \\\n",
    "    .load(os.path.join(D_RAW_DIR, 'input_file', SOURCE_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ba094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting necessary columns from page, so title and text and filtering importatn pages\n",
    "from pyspark.sql.functions import col\n",
    "dfs_valuable = dfs.select([\"title\", \"revision.text._VALUE\"]) \\\n",
    "    .filter(~col(\"revision.text._VALUE\").like('#REDIRECT%')) \\\n",
    "    .filter(col(\"revision.text._VALUE\").rlike('(?i)bacteria|(?i)virus|(?i)viral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Defining udf for finding matches between page and item(disease, symptom, transmission, country)\n",
    "def get_matching_string(line, regex):\n",
    "    matches = list(set([str(x).lower() for x in re.findall(regex, line)]))\n",
    "    return matches if matches else None\n",
    "\n",
    "# Defining udf for finding matches between page and disease type\n",
    "def get_matching_string_type(line, regex):\n",
    "    disease_types = re.findall(regex, line)    \n",
    "    match = max([x.lower().replace(\"viral\", \"virus\") for x in disease_types], key=disease_types.count) \n",
    "    return match if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbccdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring UDF functions\n",
    "from pyspark.sql.functions import udf\n",
    "udf_func_type = udf(lambda line :get_matching_string_type(line, diseases_type_pattern), StringType())\n",
    "udf_func_diseases = udf(lambda line :get_matching_string(line, diseases_pattern), ArrayType(StringType()))\n",
    "udf_func_symptoms = udf(lambda line :get_matching_string(line, symptoms_pattern), ArrayType(StringType()))\n",
    "udf_func_countries = udf(lambda line :get_matching_string(line, countries_pattern), ArrayType(StringType()))\n",
    "udf_func_transmissions = udf(lambda line :get_matching_string(line, transmission_pattern), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6fcd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating collumns disease, type, symptoms, countries, transmissions and dropping yet \n",
    "# unnecessary columns _VALUE and title\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "dfs_final = dfs_valuable \\\n",
    "                .withColumn(\"disease\", udf_func_diseases(col('title'))[0]) \\\n",
    "                .na.drop(subset=[\"disease\"]) \\\n",
    "                .withColumn(\"type\", udf_func_type('_VALUE')) \\\n",
    "                .withColumn(\"symptoms\", udf_func_symptoms(col('_VALUE'))) \\\n",
    "                .withColumn(\"countries\", udf_func_countries(col('_VALUE'))) \\\n",
    "                .withColumn(\"transmissions\", udf_func_transmissions(col('_VALUE'))) \\\n",
    "                .drop(col(\"_VALUE\")) \\\n",
    "                .drop(col(\"title\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12c158",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for row in dfs_final.rdd.collect():\n",
    "    # Getting id of row from main document to know, which line(disease) will be processed\n",
    "    id = main_df[main_df['Disease'].str.contains(row.disease.replace(\"(\", \"\\(\").replace(\")\", \"\\)\"), flags=re.IGNORECASE)].index[0]\n",
    "\n",
    "    # Writing type of disease to main document\n",
    "    main_df.at[id, 'Type'] = f\"{row.type}\"\n",
    "\n",
    "    # Writing symptoms of disease to main document\n",
    "    if row.symptoms != None:\n",
    "        for symptom in row.symptoms:\n",
    "            if main_df.at[id, 'Symptom'] == \"\":\n",
    "                main_df.at[id, 'Symptom'] = f\"{str(symptom)}\"\n",
    "            else:\n",
    "                if symptom not in main_df['Symptom'][id]:\n",
    "                    main_df.at[id, 'Symptom'] = f\"{main_df['Symptom'][id]}; {str(symptom)}\"\n",
    "\n",
    "    # Writing countries of disease spread to main document \n",
    "    if row.countries != None:\n",
    "        for country in row.countries:\n",
    "            if main_df.at[id, 'Country'] == \"\":\n",
    "                main_df.at[id, 'Country'] = f\"{str(country)}\"\n",
    "            else:\n",
    "                #print(type(main_df['Country'][id]))\n",
    "                if country not in main_df['Country'][id]:\n",
    "                    main_df.at[id, 'Country'] = f\"{main_df['Country'][id]}; {str(country)}\"\n",
    "\n",
    "    # Writing transmissions of disease to main document    \n",
    "    if row.transmissions != None:\n",
    "        for transmission in row.transmissions:\n",
    "            if main_df.at[id, 'Transmission'] == \"\":\n",
    "                main_df.at[id, 'Transmission'] = f\"{str(transmission)}\"\n",
    "            else:\n",
    "                if transmission not in main_df['Transmission'][id]:\n",
    "                    main_df.at[id, 'Transmission'] = f\"{main_df['Transmission'][id]}; {str(transmission)}\"         \n",
    "\n",
    "\n",
    "\n",
    "    main_df.to_csv(main_document_path, sep='\\t', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinfenv_kernel",
   "language": "python",
   "name": "vinfenv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
